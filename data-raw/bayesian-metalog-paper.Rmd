---
title: "Metalog likelihoods in Bayesian models"
description: |
  A new article created using the Distill format.
author:
  - first_name: "Dmytro"
    last_name: "Perepolkin"
    url: https://ddrive.no/
    affiliation: CEC, Lund University
    affiliation_url: https://www.cec.lu.se/
    orcid_id: 0000-0001-8558-6183
date: "`r Sys.Date()`"
bibliography: "`r rbbt::bbt_write_bib('bayesian-metalog-paper.bib', overwrite = TRUE)`"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
what we want to accomplish
- elicit predictive distribution with some uncertainty in quantiles (2d prior predictive)
- get new data
- use it to update the belief about the uncertainty in the quantiles.

## Glossary

**Cumulative distribution function (CDF)** denoted by $F_X(x|\theta)$ is defined as probability of the random variable $X$ being less than or equal to some given value $x$.
$$F_X(x|\theta) = P(X \leq x|\theta)$$
**Probability density function (PDF)**, denoted by $f_X(x)$ is such that: 
$$f_X(x|\theta)dx=P(x\leq X \leq x+dx|\theta)=F_X(x+dx|\theta)-F_X(x|\theta)=\frac{dF_X(x|\theta)}{dx}$$
where $dx$ is infinitesimally small range of $x$. PDF is the first derivative of CDF. The area under the PDF curve $f_X(x)$ is equal to 1.

**Quantile function (QF)**, denoted by $Q_X(u|\theta)$ is related to the probability $u$ defined as $u \text{ for which } P(X \leq u|\theta)=u$. The function $Q_X(u|\theta)$ is called a *quantile function* and it expresses *u-quantile* as a function of *u*. 

$$Q_X(u|\theta)=inf\{u:F_X(x|\theta)\geq u\}, \quad 0 \leq u \leq 1$$

For any pair of values along the CDF $(x,u)$ it can be expressed as $x=Q(u)$ and $u=F(x)$, therefore these functions are inverses of each other $Q(u)=F^{-1}(u)$ and $F(x)=Q^{-1}(x)$. When CDF $F$ is continuous and strictly increasing (proper CDF) with quantile function $Q$, then $F(Q(u))=u$.

**Quantile density function (QDF)** denoted by $q_X(u)$ is the first derivative of the Quantile Function (QF), the same way as PDF is a first derivative of CDF.
$$q_X(u)=\frac{dQ_X(u|\theta)}{du}$$
Because $q_X(u)$ is the slope of QF, it is non-negative on $0 \leq u \leq 1$.

The advantage of quantile functions is that "quantile functions can be added and, in the right circumstances, multiplied to obtain new quantile functions. Quantile density functions can also be added together to derive new quantile density functions" [@gilchrist2000StatisticalModellingQuantile].

**Reciprocal quantile density function (RQDF)** also known as *p-pdf* [@gilchrist2000StatisticalModellingQuantile] is, as per definition, reciprocal of $q_X(u)$. It is also representing the probability density of QF.

$$f(Q(u))=\frac{dF(Q(u))}{dQ(u)} = \frac{dF(Q(u))/du}{dQ(u)/du}=\frac{dF(F^{-1}(u))/du}{q(u)}=\frac{du/du}{q(u)}=  (q(u))^{-1}$$ 

Therefore:
$$q(u)f(Q(u))=1$$

**Hazard rate and hazard function**. In reliability-related literature, hazard rate $h(x)$ is defined as:

$$h(x)=f(x)/\bar{F}(x))$$
where $\bar{F}(x)=1-F(x)$ is survival function (also known as reliability function or complimentary CDF) of $x$ and $F(x)$ is a distribution function (CDF).

Also, hazard quantile function

  $$H(u)=h(Q(u))=[(1-u)q(u)]^{-1}$$
To learn more about hazard quantile function see @nair2009QuantileBasedReliabilityAnalysis.

# Bayesian updating for quantile functions

## Bayesian updating 

The information about the parameter $\theta$ defined on $\theta\in A\subset\mathbb{R}$ can be described by prior distribution of random variable $\Theta$  with $F_\Theta(\theta)=v$.

Let's denote the sample of $x$ (of size $n$) from $F_X(x|\theta)$ as $\underline{x}=(x_1, x_2, ...,x_n)$. 

The posterior density $f_\Theta(\theta|\underline{x})$ is given by:

$$f_{\Theta|\underline{x}}(\theta|\underline{x})=K(\underline{x})f_X(\underline{x}|\theta)f_\Theta(\theta)$$
where $f_X(\underline{x}|\theta)=\prod_{i=1}^{n}f_X(x_i|\theta)$ is the likelihood and $K(\underline{x})$ is the normalizing constant, given by
$$(K(\underline{x}))^{-1}=\int_A f_X(\underline{x}|\theta)f_\Theta(\theta)d\theta$$
## Quantile likelihood, density prior

The posterior $f_{\Theta|\underline{x}}(\theta|\underline{x})$ can be expressed in terms of quantile function $\underline{Q}$ of the sample $\underline{x}$.

$$f_{\Theta|\underline{Q}}(\theta)=K(\underline{Q})f_X(\underline{Q}|\theta)f_\Theta(\theta)$$
where $\underline{Q}=(Q_1(u_1), Q_2(u_2), ...,Q_n(u_n))$ and $F(x_i|\theta)=u_i$ for $i=1,2,...,n$.

## Quantile likelihood, quantile prior

In case the prior $f_\Theta(\theta)$ is not readily available and can only be expressed as quantile function, we express prior in terms of quantile function as well.

Because $F_\Theta(\theta)=v$ then $Q_\Theta(v)=\theta$. For the prior $f_{\Theta|\underline{Q}}(\theta)$ it would mean

$$f_\Theta(\theta)=\frac{dF_\Theta(\theta)}{d\theta}=\frac{dF_\Theta(Q_\Theta(v))}{d(Q_\Theta(v))}=\frac{dF_\Theta(Q_\Theta(v))/dv}{d(Q_\Theta(v))/dv}=\frac{dF_\Theta(F^{-1}_\Theta(v))/dv}{q_\Theta(v)}=\frac{dv/dv}{q_\Theta(v)}=\frac{1}{q_\Theta(v)}=(q_\Theta(v))^{-1}$$

We can substitute these into the formula for the posterior:

$$f_{\Theta|\underline{Q}}(Q_\Theta(v)|\underline{Q})=K_1(\underline{Q})f_X(\underline{Q}|Q_\Theta(v))(q_\Theta(v))^{-1}$$
where $0\leq u_i, \, v\leq 1$ and $(K_1(\underline{Q}))^{-1}=\int_{0}^{1}f_X(\underline{Q}|Q_\Theta(v))(q_\Theta(v))^{-1} dv$

Therefore posterior QDF $q_{\Theta|\underline{Q}}(v)$ has the form:

$$q_{\Theta|\underline{Q}}(v)=\frac{(K_1(\underline{Q}))^{-1}q_\Theta(v)}{f(\underline{Q}|Q_\Theta(v))}=(K_1(\underline{Q}))^{-1}q_\Theta(v)\prod_{i=1}^{n}q_X(u_i|Q_\Theta(v))$$
This last equation is the quantile version of the Bayes theorem which does not require the density function of the variable $X$, nor density of parameter $\Theta$.

## Combined method of H<sup>-1</sup>(v)

Because
$$F_{\Theta|\underline{Q}}(Q_\Theta(v))=K_1(\underline{Q})\int_{0}^{v}f(\underline{Q}|Q_\Theta(p))dp$$
The right hand side of this equation can be denoted as $H(v)$ and it is a distribution on $[0,1]$ and therefore

$$Q_\Theta(v)=Q_{\Theta|\underline{Q}}(H(v))$$
Thus the posterior can be expressed as

$$Q_{\Theta|\underline{Q}}(v)=Q_\Theta(H^{-1}(v))$$
The final equation expresses the relationship between the prior and the posterior quantile function and how the prior distribution of $\theta$ changes in light of observations on $X$.

@nair2020BayesianInferenceQuantile also use alternative expression for $H(v)$:

$$H(v)=K_2\int_{0}^{u}(q(\underline{Q}|Q_{\theta}(p)))^{-1}dp$$
where $K_2^{-1}=\int_0^{1}(q(\underline{Q}|Q_{\theta}(p)))^{-1}dp$

# Converting the data to quantiles

Bayesian updating in terms of quantile function are expressed in terms of $u_i=F(x_i|\theta)$. In reality only observational values of $\underline{x}$ are known. They can be ordered so that the $r$-th order statistic can be expressed as $x_{(r)}=Q_X(u_{(r)}|\theta)$ for every $r = 1,2,...n$. In order to transition from $x_{(r)}$ to $u_{(r)}$, we can employ Taylor series approximation described by @gilchrist1997ModellingQuantileDistribution, p. 99, 210 and @nair2009QuantileBasedReliabilityAnalysis, p. 345, which is based on approximating a value of $0<u_0<1$ such that: 

$$Q_X(u_{(r)}|\theta)=x_{(r)}=Q_X(u_0|\theta)+(u_{(r)}-u_0)q_X(u_0|\theta)$$ 
Assuming $Q_X(u_{(r)}|\theta)=x_{(r)}$ and rearranging in terms of $u_{(r)}$ we get

$$u_{(r)}=\frac{x_{(r)}-Q_X(u_0|\theta)}{q_X(u_0|\theta)}+u_0$$

@gilchrist2000StatisticalModellingQuantile suggested that $r/(n + 1)$ can serve as a reasonable initial guess of $u_0$.
 
The initial $Q_X(u_0|\theta)$ can be obtained by replacing $\theta$ in it by the suitable single point-estimate $\hat{\theta}$ derived from prior distribution of $\theta$ (mean or median) [@nair2020BayesianInferenceQuantile].
 
Once $u_{(r)}$ is obtained from the equation above, $Q(u_{(r)})$ can be compared to the value of $x_{(r)}$. If the absolute difference is large, i.e. greater than certain small value $epsilon$, the procedure can be repeated again. In order to proceed to the next iteration of the approximation algorithm, the obtained value $u_{(r)}$ replaces initial guess $u_0$ and the process continues resulting in a new value of $u_{(r)}$ and so on, until $|Q(u_{(r)})-x_{(r)}|<\epsilon$. As suggested by @gilchrist2000StatisticalModellingQuantile the approximation algorithm can exhibit numerical instability for very small and very large estimates of $u_{(r)} \in (0,1)$. Therefore, values of $u_{(r)}$ less than certain very small value $\varsigma$  and more than $1- \varsigma$ can be replaced with $\varsigma$ and $1-\varsigma$, respectively to ensure numerical stability.

Once the good estimate of $u_{(r)}$ is obtained (such that $Q(u_{(r)}|\hat\theta)$ is indistinguishable from $x_{(r)}$), this value can be used in Bayesian updating formulas above. The process is repeated for every r-th order statistic (each data point in the ordered dataset) independently.

After bayesian updating, posterior predictive distribution will be also expressed in terms of $u_i$. In this case, same value of $\hat\theta$ can be used to "decode" the updated quantile probabilities back into the data scale using $Q(u|\hat\theta)$.

# Metalog distributions

@keelin2011QuantileParameterizedDistributions defined quantile-parametrized distribution as a continuous probability distribution which quantile function $Q(u)$ can be expressed as:

$$Q_{QPD}(u)=\begin{cases} \begin{aligned}&L_0, \; &u=0,\\&\sum_{i=1}^{n}a_ig_i(u), &0<u<1, \\&L_1, &u=1, \end{aligned}\end{cases}$$

with constants $L_0=\lim_{u \to 0} Q(u)$ and  $L_1=\lim_{u \to 1} Q(u)$ representing the lower and the upper limit of the quantile function, respectively; $g_i(u), \; i \in 1:n$ is a set of continously differentiateable and linearly independent functions of cumulative probability $u \in [0,1]$ and $a_i, \; i\in 1:n$ is a set of real constants.

Logistic distribution has been widely used and applied in various domains starting from early 19th century [@johnson1994ContinuousUnivariateDistributions]. It is particularly interesting due to the simplicity of its quantile function [@balakrishnan1992HandbookLogisticDistribution], which can be written out as:

$$Q_L(u|\mu,s)=\mu+s\ln\left(\frac{u}{1-u}\right)$$

where $\mu$ is mean and $s$ is proportional to the standard deviation $\sigma=s\pi/\sqrt3$. The Metalog distribution is built from the logistic quantile function by substitution and series expansion of its parameters with the polynomial of the form:

$$\mu=a_1+a_4(u-0.5)+a_5(u-0.5)^2+a_7(u-0.5)^3+a_9(u-0.5)^4+\cdots,\\
s=a_2+a_3(u-0.5)+a_6(u-0.5)^2+a_8(y-0.5)^3+a_{10}(u-0.5)^4+\cdots,$$

where $a_i, i \in 1..n$ are real constants. The shape flexibility of the distribution increases with the number of terms added into the expansion. The order of the terms $n$ is limited by the number of "QP pairs" quantile-probability pairs $m, n\leq m$ used in  and the concerns about the overfitting. Given a set of $m$ data pairs from the CDF curve $(x,u)$ the parameters $a_i$ can be determined through the set of linear equations.

$$\begin{aligned}
&x_1=a_1+a_2\text{logit}(u_1)+a_3(u_1-0.5)\text{logit}(u_1)+a_4(u_1-0.5)+\cdots,\\
&x_2=a_1+a_2\text{logit}(u_2)+a_3(u_2-0.5)\text{logit}(u_2)+a_4(u_2-0.5)+\cdots,\\
&\vdots\\
&x_m=a_1+a_2\text{logit}(u_m)+a_3(u_m-0.5)\text{logit}(u_m)+a_4(u_m-0.5)+\cdots.\\
\end{aligned}$$

where $\text{logit}(u)=\ln(u/(1-u))$ is log-odds of probability $u$. In matrix form this system of equations is equivalen to $\mathbf{x}=\mathbf{U}\mathbf{a}$, where $\mathbf{x}$ and $\mathbf{a}$ are column vectors and $\mathbf{U}$ is a $m \times n$ matrix:

$$\mathbf{U} = \left[\begin{array}
&1 &\text{logit}(u_1) &(u_1-0.5)\text{logit}(u_1) &(u_1-0.5) &\cdots\\
1 &\text{logit}(u_2) &(u_2-0.5)\text{logit}(u_2) &(u_2-0.5) &\cdots\\
& \vdots\\
1 &\text{logit}(u_m) &(u_m-0.5)\text{logit}(u_m) &(u_m-0.5) &\cdots
\end{array}\right]$$

  If $m=n$ and $\mathbf{U}$ is invertible, then $\mathbf{a}$ can be uniquely determined by $\mathbf{a}=\mathbf{U}^{-1}\mathbf{x}$ and if $m \geq n$ and $\mathbf{U}$ has a rank of at least $n$, then $\mathbf{a}$ can be estimated using 

$$\mathbf{a}=[\mathbf{U}^T\mathbf{U}]^{-1}\mathbf{U}^T\mathbf{x}
(\#eq:metalogAsMatrix)$$. 

The matrix in Equation \@ref(eq:metalogAsMatrix) to be inverted is always $n \times n$ regardless of the number of observations $m$ used. 

Once the estimates of $\mathbf{a}=a_1..a_n$ are obtained, they can be used to as parameters of the metalog distribution.

> In order to obtain estimates of $\mathbf{a}=a_1..a_n$ a minimum of $n$ QP pairs are required. These can be obtained by elicitation from experts or using quantile methods of summarizing the probability distributions. 

Metalog quantile function (QF) with $n$ terms $Q_{M_n}(u|\mathbf{a})$ can be expressed as 

$$Q_{M_n}(u|\mathbf{a})=\begin{cases}\begin{aligned}
&a_1+a_2\text{logit}(u), &\text{ for } n=2, \\
&a_1+a_2\text{logit}(u)+a_3(u-0.5)\text{logit}(u), &\text{ for } n=3, \\
&a_1+a_2\text{logit}(u)+a_3(u-0.5)\text{logit}(u)+a_4(u-0.5), &\text{ for } n=4, \\
&Q_{M_{n-1}} + a_n(u-0.5)^{(n-1)/2}, &\text{ for odd } n \geq 5, \\
&Q_{M_{n-1}} + a_n(u-0.5)^{n/2-1}\text{logit}(u), &\text{ for even } n \geq 6, \\
\end{aligned}
\end{cases}
(\#eq:metalogQF)$$

where $u \in (0,1)$ is cumulative probability and $\mathbf{a}$ is a parameter vector of real constants $a_i, i \in 1..n$. 

The metalog quantile density function (QDF) can be found by differentiating Equations \@ref(eq:metalogQF) with respect to $u$:

$$q_{M_n}(u)=\begin{cases}\begin{aligned}
&a_2(u\overline{u})^{-1}, &\text{ for } n=2, \\
&a_2(u\overline{u})^{-1} + a_3\left((u-0.5)(u\overline{u})^{-1}+\text{logit}(u) \right),  &\text{ for } n=3, \\
&a_2(u\overline{u})^{-1} + \left((u-0.5)(u\overline{u})^{-1}+\text{logit}(u) \right)+ a_4,  &\text{ for } n=3, \\
&q_{M_{n-1}} + 0.5a_n(n-1)(u-0.5)^{(n-3)/2}, &\text{ for odd } n \geq 5, \\
&q_{M_{n-1}} + a_n\left((u-0.5)^{n/2-1}(u\overline{u})^{-1}+ (0.5n-1)(u-0.5)^{n/2-2}\text{logit}(u)\right), &\text{ for even } n \geq 6, \\
\end{aligned}
\end{cases}
(\#eq:metalogQDF)$$

where $\overline{u}=1-u$ and $\text{logit}(u)=\ln(u/(1-u))$. 

@keelin2016MetalogDistributions showed that the constants $a_i, i\in 1..n$ are feasible iif $q_{M_n}(u)>0$ for all $u \in (0,1)$.

@keelin2016MetalogDistributions provides another measure of quantile density which is a reciprocal of QDF, also known as *p-pdf* $f(Q_{M_n}(u))=(q_{M_n}(u))^{-1}$.

# Q-dirichlet prior 

Math about dirichlet. Interpretation of $K$-term dirichlet as $K-1$ points along CDF to be called Q-dirichlet distribution. Adaptation of elicitation procedure for eliciting Q-dirichlet along with the index to keep track which dirichlet "bin" correspond to which "CDF gap". 

> Alternatively, we could keep Dirichlet prior as is. We would then modify metalog to be parametrized by $K$ simplex and $K-1$ quantiles, doing all of the transformations, including fitting of metalog (finding $a$ vector) inside metalog function.

# Cases

## Fish Size

## some well known bayesian case


